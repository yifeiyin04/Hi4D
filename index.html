<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Hi4D: 4D Instance Segmentation of Close Human Interaction">
  <meta name="keywords" content="Hi4D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Hi4D: 4D Instance Segmentation of Close Human Interaction</title>


  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Hi4D: 4D Instance Segmentation of Close Human Interaction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/yifeiyin04/">Yifei Yin</a>,</span>
              <span class="author-block">
              <a href="https://ait.ethz.ch/people/chen/">Chen Guo</a>,</span>
              <span class="author-block">
              <a href="https://ait.ethz.ch/people/kamanuel/">Manuel Kaufmann</a>,</span>
              <span class="author-block">
              <a href="https://ait.ethz.ch/people/jzarate/"> Juan Jose Zarate</a>,</span>
              <span class="author-block">
              <a href="https://ait.ethz.ch/people/song/">Jie Song</a>,</span>
              <span class="author-block">
              <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ETH Zürich</span>
            <!-- <span class="author-block"><sup>1</sup>ETH Zürich</span> -->
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (coming soon)</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/lZEUn-kgWhA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded ">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Dataset (coming soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="https://ait.ethz.ch/projects/2023/hi4d/downloads/assets/teaser.png"  class="center"/>
      <h2 class="subtitle has-text-centered">
        Hi4D (<u>H</u>umans <u>i</u>nteracting in <u>4D</u>), a dataset of humans in close physical interaction that contains (A) 4D textured scans, (B) instance meshes with vertex-level contact annotations, (C) instance masks in 2D and 3D, (D) registered parametric body models with contact annotations.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose Hi4D, a method and dataset for the automatic analysis of physically close human-human interaction under prolonged contact. Robustly disentangling several in-contact subjects is a challenging task due to occlusions and complex shapes. Hence, existing multi-view systems typically fuse 3D surfaces of close subjects into a single, connected mesh. To address this issue we leverage i) individually fitted neural implicit avatars; ii) an alternating optimization scheme that refines pose and surface through periods of close proximity; and iii) thus segment the fused 4D raw scans into individual instances. From these instances we compile a dataset of textured scans of 20 subject pairs (24 male, 16 female), 108 sequences and a total of more than 12k frames. Hi4D contains rich interaction centric annotations in 2D and 3D alongside accurately registered parametric body models. We define varied human pose and shape estimation tasks on this dataset and provide results from state-of-the-art methods on these benchmarks. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe width="560" height="315" src="https://youtu.be/embed/lZEUn-kgWhA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="method">
  <div class="container is-max-desktop content">
    <h2 class="title">Method</h2>
    <img src="https://ait.ethz.ch/projects/2023/hi4d/downloads/assets/pipeline.png"  height="250" class="center"/>
    <p>
      Vision-based disentanglement of in-contact subjects is a challenging task due to strong occlusions and a-priori unknown geometries. Hence, multi-view systems typically fuse 3D surfaces of close subjects into a single, connected mesh. In this paper, we aim to segment 4D scans of closely interacting people to obtain instance-level annotations. Our method makes use of two main components: 
      <ul style="list-style: none; padding-left: 0">
        <li><strong>(A) Dynamic Personalized Priors:</strong> We build individual personalized implicit avatars from 4D posed scans of each subject by modelling shape and deformation fields in canonical space; </li>
        <li><strong>(B) Instance Segmentation During Interaction:</strong> We then leverage the pre-built individual avatars to track and segment the raw 4D scans of multiple closely interacting people through extended periods of dynamic physical contact by optimizing pose and shape in an alternating manner.
        </li>
      </ul>
    </p>
  </div>
</section>

<section class="section" id="result">
  <div class="container is-max-desktop content">
    <h2 class="title">Dataset</h2>
    <p>Hi4D is the first dataset containing rich interaction centric annotations and high-quality 4D textured geometry of closely interacting humans. The dataset contains:</p>
    <ul> 
      <li>4D textured scans</li>
      <li>Instance segmentation masks in 2D and 3D</li>
      <li>Parametric body models</li>
      <li>Vertex-level contact annotations</li>
      <li>Multi-view RGB videos</li>

    </ul>

     <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2023/hi4d/downloads/assets/texture_scan.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>

      <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2023/hi4d/downloads/assets/instance_meshes.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2023/hi4d/downloads/assets/instance_masks.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video autoplay controls muted loop height="100%">
            <source src="https://ait.ethz.ch/projects/2023/hi4d/downloads/assets/smpl.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section" id="SynWild Dataset">
  <div class="container is-max-desktop content">
    <h2 class="title">Download</h2>
    <p>
      Download instructions will be available soon.    
    </p>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{yin2023hi4d,
      title={Hi4D: 4D Instance Segmentation of Close Human Interaction},
      author={Yin, Yifei and Guo, Chen and Kaufmann, Manuel and Zarate, Juan and Song, Jie and Hilliges, Otmar},    
      journal   = {Computer Vision and Pattern Recognition (CVPR)},
      year      = {2023}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://ait.ethz.ch/projects/2023/vid2avatar/downloads/main.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/MoyGcc/vid2avatar" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage is built with the template from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>. We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
